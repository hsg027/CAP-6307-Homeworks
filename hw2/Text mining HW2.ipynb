{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import spacy\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import time\n",
    "from __future__  import unicode_literals\n",
    "from io import open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_time= time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "hockey_train = \"C:/Users/harsh/Desktop/ucf academics/Sem-3/6307- Text Mining/HW2/20news-bydate.tar/20news-bydate-train/rec.sport.hockey\"\n",
    "auto_train = \"C:/Users/harsh/Desktop/ucf academics/Sem-3/6307- Text Mining/HW2/20news-bydate.tar/20news-bydate-train/rec.autos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "hockey_test= \"C:/Users/harsh/Desktop/ucf academics/Sem-3/6307- Text Mining/HW2/20news-bydate.tar/20news-bydate-test/rec.sport.hockey\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_test= \"C:/Users/harsh/Desktop/ucf academics/Sem-3/6307- Text Mining/HW2/20news-bydate.tar/20news-bydate-test/rec.autos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_feat = {}\n",
    "words_uniq = {}\n",
    "dic_feat = {}\n",
    "k, j = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabolary list using training set and tokenise words using space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 0\n",
    "for nr_pos, name_file in enumerate(os.listdir(hockey_train)):\n",
    "    doc_file = nlp(open(hockey_train + \"/\" + name_file, encoding=\"ISO-8859-1\").read())\n",
    "    pos += 1\n",
    "    for token in doc_file:\n",
    "        if token.text.isalpha() and not token.is_stop:\n",
    "            if token.tag_ not in words_uniq:\n",
    "                words_uniq[token.pos_] = j\n",
    "                j += 1\n",
    "            if token.lemma_ not in counts_feat:\n",
    "                counts_feat[token.lemma_] = 0\n",
    "                dic_feat[token.lemma_] = k\n",
    "                k += 1\n",
    "            else:\n",
    "                counts_feat[token.lemma_] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = 0\n",
    "for nr_neg, file_name in enumerate(os.listdir(auto_train)):\n",
    "    doc_file = nlp(open(auto_train + \"/\" + file_name, encoding=\"ISO-8859-1\").read())\n",
    "    neg += 1\n",
    "    for token in doc_file:\n",
    "        if token.text.isalpha() and not token.is_stop:\n",
    "            if token.tag_ not in words_uniq:\n",
    "                words_uniq[token.pos_] = j\n",
    "                j += 1\n",
    "            if token.lemma_ not in counts_feat:\n",
    "                counts_feat[token.lemma_] = 0\n",
    "                dic_feat[token.lemma_] = k\n",
    "                k += 1\n",
    "            else:\n",
    "                counts_feat[token.lemma_] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_feat = {}\n",
    "k = 0\n",
    "for key in counts_feat.keys():\n",
    "    dic_feat[key] = k\n",
    "    k += 1\n",
    "\n",
    "features_total = len(counts_feat)\n",
    "train_instances = pos + neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.zeros((train_instances, features_total), dtype=float)\n",
    "train_y = np.vstack((np.zeros([pos, 1], dtype=int), np.ones([neg, 1], dtype=int)))\n",
    "\n",
    "pos = 0\n",
    "for nr_pos, file_name in enumerate(os.listdir(hockey_train)):\n",
    "    doc_file = nlp(open(hockey_train + '/' + file_name, encoding=\"ISO-8859-1\").read())\n",
    "    for token in doc_file:\n",
    "        if token.text.isalpha() and not token.is_stop:\n",
    "            if token.lemma_ in dic_feat:\n",
    "                train_x[pos, dic_feat[token.lemma_]] += 1\n",
    "    pos += 1\n",
    "\n",
    "neg = 0\n",
    "for nr_neg, file_name in enumerate(os.listdir(auto_train)):\n",
    "    doc_file = nlp(open(auto_train + '/' + file_name, encoding=\"ISO-8859-1\").read())\n",
    "    for token in doc_file:\n",
    "        if token.text.isalpha() and not token.is_stop:\n",
    "            if token.lemma_ in dic_feat:\n",
    "                train_x[pos + neg, dic_feat[token.lemma_]] += 1\n",
    "    neg += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = len(os.listdir(hockey_test)) #no. of test data for hockey (positive)\n",
    "neg = len(os.listdir(auto_test)) #no. of test data for autos (negative)\n",
    "\n",
    "test = pos + neg\n",
    "\n",
    "test_x = np.zeros([test, total_features], dtype=float)\n",
    "test_y = np.vstack((np.zeros([pos, 1], dtype=int), np.ones([neg, 1], dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test matrix creation using vocab list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 0\n",
    "for nr_pos, file_name in enumerate(os.listdir(hockey_test)):\n",
    "    doc_file = nlp(open(hockey_test + \"/\" + file_name, encoding=\"ISO-8859-1\").read())\n",
    "    for token in doc_file:\n",
    "        if token.text.isalpha() and not token.is_stop:\n",
    "            if token.lemma_ in dic_feat:\n",
    "                test_x[pos, dic_feat[token.lemma_]] += 1\n",
    "    pos += 1\n",
    "\n",
    "neg = 0\n",
    "for nr_neg, file_name in enumerate(os.listdir(auto_test)):\n",
    "    doc_file = nlp(open(auto_test + \"/\" + file_name, encoding=\"ISO-8859-1\").read())\n",
    "    for token in doc_file:\n",
    "        if token.text.isalpha() and not token.is_stop:\n",
    "            if token.lemma_ in dic_feat:\n",
    "                test_x[pos + neg, feat_dict[token.lemma_]] += 1\n",
    "    neg += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_new = np.arange(test)\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(order_new)\n",
    "test_x, test_y = test_x[order_new, :], test_y[order_new, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive bayes classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Hockey(class 0)    1.00000   0.98246   0.99115       399\n",
      "  Auto(class 1)    0.98263   1.00000   0.99124       396\n",
      "\n",
      "      micro avg    0.99119   0.99119   0.99119       795\n",
      "      macro avg    0.99132   0.99123   0.99119       795\n",
      "   weighted avg    0.99135   0.99119   0.99119       795\n",
      "\n",
      "F1 score is : 0.9911949685534591\n",
      "()\n",
      "Total number of test documents :795\n",
      "Total number of train documents :1194\n",
      "Total Time for Execution:5932.92400002\n",
      "Total number of unique words:15\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(train_x, train_y)\n",
    "pred_y = clf.predict(test_x)\n",
    "row, column = test_y.shape\n",
    "target_names = ['Hockey(class 0)', 'Auto(class 1)']\n",
    "print(classification_report(test_y, pred_y, target_names=target_names, digits=5))\n",
    "print(\"F1 score is : \" + str(f1_score(test_y, pred_y, average='micro')))\n",
    "print()\n",
    "print('Total number of test documents :' + str(test))\n",
    "print('Total number of train documents :' + str(train_instances))\n",
    "print(\"Total Time for Execution:\" + str(time.time() - begin_time))\n",
    "print(\"Total number of unique words:\" + str(len(words_uniq)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
